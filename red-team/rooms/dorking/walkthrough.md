## Task 1 ‚Äì Ye Ol‚Äô Search Engine

*No answer needed.*

This task provides an overview of how search engines index and crawl the web ‚Äî laying the groundwork for understanding later tasks.

## Task 2 ‚Äì Let‚Äôs Learn About Crawlers

**Question 1:** Name the key term of what a ‚ÄúCrawler‚Äù is used to do
**Answer:** `index`

**Question 2:** What is the name of the technique that ‚ÄúSearch Engines‚Äù use to retrieve this information about websites?
**Answer:** `crawling`

**Question 3:** What is an example of the type of contents that could be gathered from a website?
**Answer:** `keywords`

**Explanation:** Web crawlers (also called spiders or bots) systematically browse the web to discover URLs and collect information. They **index** web pages, enabling search engines to respond to user queries. During crawling, crawlers gather **keywords**, URLs, and domain data to build a searchable index. :contentReference[oaicite:6]{index=6}

---

### üìé Sidebar: Do Search Engines Still Work This Way?

Yes ‚Äî despite all the modern AI enhancements, the foundation of search engines like **Google** is still based on:

1. **Crawling** ‚Äì Bots (like `Googlebot`) explore web pages via links.
2. **Indexing** ‚Äì Extracted content (text, metadata, images) is stored in a searchable index.
3. **Ranking** ‚Äì When a user searches, Google ranks results from the index using hundreds of signals (like relevance, freshness, and mobile-friendliness).

#### üß† What‚Äôs Changed?

Modern search engines go beyond keywords:

| Traditional Search                | Modern Enhancements                            |
|----------------------------------|-------------------------------------------------|
| Keyword-based indexing           | NLP-based understanding (e.g., BERT, MUM)       |
| Manual crawl discovery           | Auto-discovery via sitemaps and schema.org      |
| Link-focused ranking (PageRank)  | Machine learning + user behavior                |
| Static HTML analysis             | Dynamic rendering with JavaScript execution     |

So while crawling and indexing are still critical, Google now **understands meaning and context**, executes JavaScript, and uses **AI models** to match intent.

#### üîê Relevance to Google Dorking

Google Dorking works *because* this index is so rich. You‚Äôre querying Google's already-collected data using advanced operators like:

- `filetype:`
- `site:`
- `inurl:`
- `intitle:`
- `cache:`

That‚Äôs why pages blocked by `robots.txt` or protected by `noindex` don‚Äôt show up in dork results ‚Äî they‚Äôre never indexed in the first place.

---
## Task 3 ‚Äì Search Engine Optimization (SEO)

*No answer required.*

This task introduces SEO (Search Engine Optimization) and explains how search engines use various signals to rank pages in their results.

### Key Takeaways:
- **SEO** is about making a site easier to crawl and rank.
- Factors include:
  - Site responsiveness across browsers and devices
  - The presence of a sitemap
  - Keyword relevance
- Sites are scored by complex, proprietary algorithms.
- Tools like Google‚Äôs SEO Analyzer can show how well a site ranks.

The task ends by introducing a limitation tool ‚Äî `robots.txt` ‚Äî which will be covered in the next task.

This sets the stage for Task 4, which introduces *robots.txt* and how web admins can control what gets indexed.

Absolutely ‚Äî here‚Äôs the **complete Joplin-compatible walkthrough for Task 4 ‚Äì Robots.txt**, including all questions, corrected answers, explanations, and your requested sidebar.

---

```markdown
## Task 4 ‚Äì Robots.txt

This task explains how the `robots.txt` file controls web crawler behavior. It's located in the root directory of a site and defines what search engine crawlers (user-agents) are allowed or disallowed from indexing.

---

> **1. Where would `robots.txt` be located on the domain `ablog.com`?**
**Answer:** `ablog.com/robots.txt`

> **2. If a website was to have a sitemap, where would that be located?**
**Answer:** `/sitemap.xml`

> **3. How would we only allow "Bingbot" to index the website?**
**Answer:**

`User-agent: Bingbot
Allow: /`


> **4. How would we prevent a "Crawler" from indexing the directory `/dont-index-me/`?**
**Answer:** `Disallow: /dont-index-me/`

> **5. What is the extension of a Unix/Linux system configuration file that we might want to hide from "Crawlers"?**
**Answer:** `.conf`

---

### üß† Explanation

- `robots.txt` lives at the root of a domain (`example.com/robots.txt`).
- It provides indexing rules for crawlers, with instructions like:
  - `User-agent`: specifies which crawler the rule applies to.
  - `Allow`: explicitly allows access.
  - `Disallow`: blocks access to directories or files.
- You can even use patterns to block entire file types (e.g., `*.conf`).
- This file is **public** ‚Äî attackers often read it for hints about sensitive directories.

---

### üìé Sidebar: What if There Is No `robots.txt`?

If a website does **not** include a `robots.txt` file at its root, search engine crawlers will assume **full permission** to index everything they can reach.

#### üîç Default Behavior
- No file = no restrictions.
- Crawlers will index all accessible URLs, unless blocked by other means (like authentication or `noindex` tags).

#### üîê Security Implication
This can lead to sensitive directories (e.g., `/admin`, `/config`, `/backups`) being indexed and appearing in Google search results ‚Äî a common Google Dorking target.

üõë **Important:**
`robots.txt` is not a security control. Anyone (including attackers) can read it. Use it to avoid indexing, not to protect sensitive data.

## Task 5 ‚Äì Sitemaps

> 1. What is the typical file structure of a "Sitemap"?  
**Answer:** `.xml`

> 2. What real life example can "Sitemaps" be compared to?  
**Answer:** `map`

> 3. Name the keyword for the path taken for content on a website  
**Answer:** `route`

### üß† Explanation

- **Sitemaps** are `.xml` files that list all the important URLs on a website.
- They help search engines **crawl** the site efficiently, without relying solely on link discovery.
- The analogy of a real-world **map** fits perfectly ‚Äî just as maps show routes between places, sitemaps show routes between content.
- Crawlers use these pre-defined **routes** to locate and index content faster and more accurately.

Sitemaps are an important part of SEO because they improve how thoroughly and efficiently a site is indexed.
## Task 6 ‚Äì Using Google for Advanced Searching

> 1. What would be the format used to query the site `bbc.co.uk` about flood defences?  
**Answer:** `site:bbc.co.uk flood defences`

> 2. What term would you use to search by file type?  
**Answer:** `filetype:`

> 3. What term can we use to look for login pages?  
**Answer:** `intitle:login`

### üß† Explanation

This task introduces common Google Dorking search operators:

- `site:` restricts results to a specific domain  
  Example: `site:bbc.co.uk`
- `filetype:` filters results by file extension  
  Example: `filetype:pdf`, `filetype:xlsx`, etc.
- `intitle:` finds pages with specific terms in the title  
  Example: `intitle:login` often reveals login panels or admin pages

These operators allow you to surgically extract useful content from the web ‚Äî often content that was never meant to be easily found.

‚ö†Ô∏è **Important Reminder:**  
Google Dorking is legal because it only returns publicly indexed content. What you *do* with that content determines legality and ethics.

